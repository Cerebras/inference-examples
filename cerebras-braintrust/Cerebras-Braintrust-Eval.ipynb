{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging, evaluating, and tracing Cerebras models with Braintrust\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's install some dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install autoevals braintrust openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Braintrust knows how to intercept calls to the `openai` client library to automatically trace them. Since Cerebras has an OpenAI-compatible API, it's a breeze to set this up!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "import braintrust\n",
    "\n",
    "client = braintrust.wrap_openai(\n",
    "    openai.OpenAI(\n",
    "        api_key=os.getenv(\"CEREBRAS_API_KEY\"),\n",
    "        base_url=\"https://api.cerebras.ai/v1\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "To log to Braintrust, simply initialize a logger. All Cerebras model calls will be automatically traced and logged to Braintrust. This works for both streaming and non-streaming calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Nevada is Carson City.\n"
     ]
    }
   ],
   "source": [
    "braintrust.init_logger(\"Cerebras test\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.1-8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of the Nevada?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Braintrust, we'll see the completion along with a bunch of metrics. Wow, Cerebras is fast!\n",
    "\n",
    "![Log view](./assets/Log-view.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enter your Cerebras API key in Braintrust (under Settings -> AI providers), you can also reproduce the call in the UI, and even tweak the prompt!\n",
    "\n",
    "![Tweak prompt](./assets/Tweak-prompt.gif)\n",
    "\n",
    "## Evaluating\n",
    "\n",
    "Evals automatically support Cerebras models as well. Let's run a simple math test eval and see how it does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment add-bt-1727847550 is running at https://www.braintrust.dev/app/braintrustdata.com/p/Cerebras%20test/experiments/add-bt-1727847550\n",
      "Cerebras test (data): 2it [00:00, 25420.02it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b2dfd9d2f444a682e93879d23738dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cerebras test (tasks):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "50.00% 'Factuality' score\n",
      "\n",
      "0.18s duration\n",
      "0.17s llm_duration\n",
      "27.50tok prompt_tokens\n",
      "10tok completion_tokens\n",
      "37.50tok total_tokens\n",
      "0.00$ estimated_cost\n",
      "\n",
      "See results for add-bt-1727847550 at https://www.braintrust.dev/app/braintrustdata.com/p/Cerebras%20test/experiments/add-bt-1727847550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import Eval\n",
    "from autoevals import Factuality\n",
    "\n",
    "await Eval(\n",
    "    \"Cerebras test\",\n",
    "    data=[\n",
    "        {\"input\": \"What is 100-94?\", \"expected\": \"6\"},\n",
    "        {\"input\": \"square root of 16?\", \"expected\": \"4\"},\n",
    "    ],\n",
    "    task=lambda input: client.chat.completions.create(\n",
    "        model=\"llama3.1-8b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": input},\n",
    "        ],\n",
    "    )\n",
    "    .choices[0]\n",
    "    .message.content,\n",
    "    # We'll use the smarter Llama 3.1-70b model to evaluate the output.\n",
    "    scores=[Factuality(model=\"llama3.1-70b\", api_key=os.environ[\"CEREBRAS_API_KEY\"])],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the output is getting penalized for containing an explanation of how to solve the problem. Let's tweak the prompt and try again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment add-bt-1727847722 is running at https://www.braintrust.dev/app/braintrustdata.com/p/Cerebras%20test/experiments/add-bt-1727847722\n",
      "Cerebras test (data): 2it [00:00, 1514.19it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b5e9b8ac4d4cb8bbe93a280b512b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cerebras test (tasks):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "add-bt-1727847722 compared to add-bt-1727847550:\n",
      "100.00% (+50.00%) 'Factuality' score\t(2 improvements, 0 regressions)\n",
      "\n",
      "25.38s (+2519.12%) 'duration'         \t(0 improvements, 2 regressions)\n",
      "25.37s (+2519.73%) 'llm_duration'     \t(0 improvements, 2 regressions)\n",
      "36.50tok (+900.00%) 'prompt_tokens'    \t(0 improvements, 2 regressions)\n",
      "2tok (-800.00%) 'completion_tokens'\t(2 improvements, 0 regressions)\n",
      "38.50tok (+100.00%) 'total_tokens'     \t(0 improvements, 1 regressions)\n",
      "0.00$ (+00.00%) 'estimated_cost'   \t(0 improvements, 0 regressions)\n",
      "\n",
      "See results for add-bt-1727847722 at https://www.braintrust.dev/app/braintrustdata.com/p/Cerebras%20test/experiments/add-bt-1727847722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import Eval\n",
    "from autoevals import Factuality\n",
    "\n",
    "await Eval(\n",
    "    \"Cerebras test\",\n",
    "    data=[\n",
    "        {\"input\": \"What is 100-94?\", \"expected\": \"6\"},\n",
    "        {\"input\": \"square root of 16?\", \"expected\": \"4\"},\n",
    "    ],\n",
    "    task=lambda input: client.chat.completions.create(\n",
    "        model=\"llama3.1-8b\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Solve the problem and provide the answer only.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": input},\n",
    "        ],\n",
    "    )\n",
    "    .choices[0]\n",
    "    .message.content,\n",
    "    # We'll use the smarter Llama 3.1-70b model to evaluate the output.\n",
    "    scores=[Factuality(model=\"llama3.1-70b\", api_key=os.environ[\"CEREBRAS_API_KEY\"])],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! It looks like we improved both cases.\n",
    "\n",
    "![Updated eval](./assets/Eval-2.gif)\n",
    "\n",
    "## Where to go from here\n",
    "\n",
    "Now that you can build logs and evaluations for your Cerebras models, you can ship applications with the confidence that you can reproduce user issues,\n",
    "eval to improve your prompts, and continue to iterate with confidence.\n",
    "\n",
    "To learn more about Braintrust, check out the [docs](https://braintrust.dev/docs).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
